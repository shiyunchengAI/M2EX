{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture --no-stderr\n",
                "%pip install -U --quiet langchain_openai langsmith langgraph langchain numexpr langchainhub sqlalchemy langchain-communityc datetime matplotlib  torch==2.0.0, torchvision==0.15.1 transformer sentence-transformers[train]==3.0.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import getpass\n",
                "import os\n",
                "import logging\n",
                "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "\n",
                "def _set_if_undefined(var: str):\n",
                "    if not os.environ.get(var):\n",
                "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
                "\n",
                "_set_if_undefined(\"OPENAI_API_KEY\")\n",
                "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
                "# _set_if_undefined(\"TAVILY_API_KEY\")\n",
                "# # Optional, add tracing in LangSmith\n",
                "\n",
                "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
                "os.environ[\"LANGCHAIN_PROJECT\"] = \"XMODE-artWork\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "sys.path.append(os.path.dirname(os.getcwd()))\n",
                "# sys.path.append(os.path.dirname(os.getcwd()) + '/src')\n",
                "# sys.path.append(os.path.dirname(os.getcwd()) + '/tools')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.path.dirname(os.getcwd())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Sequence\n",
                "\n",
                "from langchain import hub\n",
                "from langchain_core.language_models import BaseChatModel\n",
                "from langchain_core.messages import (\n",
                "    BaseMessage,\n",
                "    FunctionMessage,\n",
                "    HumanMessage,\n",
                "    SystemMessage,\n",
                ")\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.runnables import RunnableBranch\n",
                "from langchain_core.tools import BaseTool\n",
                "from langchain_openai import ChatOpenAI\n",
                "from src.output_parser import M3LXPlanParser, Task\n",
                "\n",
                "from src.utils import _get_db_schema\n",
                "\n",
                "\n",
                "from tools.backend.image_qa import VisualQA\n",
                "from tools.SQL import get_text2SQL_tools\n",
                "from tools.visual_qa import get_image_analysis_tools\n",
                "from tools.plot import get_plotting_tools\n",
                "from tools.data import get_data_preparation_tools\n",
                "\n",
                "\n",
                "model=\"gpt-4o\" #gpt-4-turbo-preview\n",
                "db_path=\"/home/ubuntu/workspace/XMODE/ArtWork/art.db\"\n",
                "temperature=0\n",
                "\n",
                "LOG_PATH=\"/home/ubuntu/workspace/XMODE/ArtWork/log_temp\"\n",
                "## Tools\n",
                "vqa_model = VisualQA()\n",
                "translate= get_text2SQL_tools(ChatOpenAI(model=model, temperature=temperature), db_path)\n",
                "image_analysis = get_image_analysis_tools(vqa_model)\n",
                "data_preparation= get_data_preparation_tools(ChatOpenAI(model=model, temperature= temperature),log_path=LOG_PATH)\n",
                "plotting= get_plotting_tools(ChatOpenAI(model=model, temperature= temperature),log_path=LOG_PATH)\n",
                "\n",
                "tools = [translate,image_analysis,data_preparation,plotting]\n",
                "\n",
                " # For each image analysis task, generate three distinct questions that each convey the same idea in different wording.\n",
                "\n",
                "prompt = ChatPromptTemplate.from_messages(\n",
                "[\n",
                "(\"system\",'''Given a user question, a database schema and tools descriptions, analyze the question to identify and break it down into relevant sub-questions. \n",
                "    Determine which tools (e.g., text2SQL, image_analysis, data_preparation, plotting) are appropriate for answering each sub-question based on the available database information and tools.\n",
                "    Decompose the user question into sub_questions that capture all elements of the question’s intent. This includes identifying the main objective, relevant sub-questions, necessary background information, assumptions, and any secondary requirements. \n",
                "    Ensure that no part of the original question’s intent is omitted, and create a list of individual steps to answer the question fully and accurately using tools. \n",
                "    You may need to use one tool multiple times to answer to the original question.\n",
                "    First, you should begin by thoroughly analyzing the user's main question. It’s important to understand the key components and objectives within the query.\n",
                "    Next, you must review the provided database schema. This involves examining the tables, fields, and relationships within the database to identify which parts of the schema are relevant to the user’s question, and create a text2SQL sub-question.\n",
                "    For each sub-question, provide all the required information that may required in other tasks. In order to find this information look at the user question and the database inforamtion.\n",
                "    Ensure you include all necessary information, including columns used for filtering in the retrieve part of the database related task (i.e. Text2SQL), especially when the user question involves plotting or data exploration.\n",
                "    In sub_question related to text2SQL include all requested information to be retrieved at once.\n",
                "    In cases where the user’s question involves data that is not directly available in the database schema —such as when there is no corresponding table or column for the required information or image analysis— you must consider the need for image analysis using the image_analysis tools. \n",
                "    For instance, if the question involves comparision of image, or asking for specific objects or a object, concepts or a concepts which is not found in the database schema, you must retrieve the `imag_path` and call image analysis task, (e.g, which image depicts <X>).\n",
                "    This ensures we can address parts of the question that rely on visual data.\n",
                "    Try to include data_preparation to provide response to user questions in proper way.\n",
                "    In case the question asked for plotting, you must include first data_preparation and then the data_plotting tools.\n",
                "    With a clear understanding of the question and the database schema, you can now break down the main question into smaller, more manageable sub-questions. \n",
                "    These sub-questions should each target a specific aspect of the main question. \n",
                "    After identifying the sub-questions, you should determine the most appropriate tools to answer each one. Depending on the nature of the sub-questions, we might use a variety of tools.\n",
                "    Each sub-question should be a textual question. Dont generate a code as a sub-question.\n",
                "    In any database retreival task, retieve any other columns that may requir for next tasks, especially when the user question involves plotting or data exploration.\n",
                "    Create a plan to solve it with the utmost parallelizability. \n",
                "    Each plan should comprise an action from the following  {num_tools} types:\n",
                "{tool_descriptions}\n",
                "{num_tools}. \n",
                "join(): Collects and combines results from prior actions.\n",
                "\n",
                "- An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
                "- join should always be the last action in the plan, and will be called in two scenarios:\n",
                "(a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
                "(b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
                "- Each action described above contains input/output types and description.\n",
                "- You must strictly adhere to the input and output types for each action.\n",
                "- The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
                "- Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
                "- Each action MUST have a unique ID, which is strictly increasing.\n",
                "- Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
                "- If there is an input from from preceding actions, always point its id as `$id` in the context of the action/\n",
                "- Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
                "- Ensure the plan maximizes parallelizability.\n",
                "- Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
                "- Never introduce new actions other than the ones provided.'''),\n",
                "    (\"user\", '{messages}'),\n",
                "    (\"assistant\", 'Remember, ONLY respond with the task list in the correct format! E.g.:\\nidx. tool(arg_name=args)'),\n",
                "\n",
                "]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_planner(\n",
                "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate, database_schema:str=None\n",
                "):\n",
                "    tool_descriptions = \"\\n\".join(\n",
                "        f\"{i+1}. {tool.description}\\n\"\n",
                "        for i, tool in enumerate(\n",
                "            tools\n",
                "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
                "    )\n",
                "    planner_prompt = base_prompt.partial(\n",
                "        replan=\"\",\n",
                "        num_tools=len(tools)\n",
                "        + 1,  # Add one because we're adding the join() tool at the end.\n",
                "        tool_descriptions = tool_descriptions,\n",
                "        #database_schema=database_schema,\n",
                "    )\n",
                "    replanner_prompt = base_prompt.partial(\n",
                "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
                "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
                "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
                "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
                "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
                "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
                "        num_tools = len(tools) + 1,\n",
                "        tool_descriptions=tool_descriptions,\n",
                "        #database_schema=database_schema,\n",
                "    )\n",
                "\n",
                "    def should_replan(state: list):\n",
                "        # Context is passed as a system message\n",
                "        return isinstance(state[-1], SystemMessage)\n",
                "\n",
                "    def wrap_messages(state: list):\n",
                "        # print(\"wrap_messages state:\", state)\n",
                "        return {\"messages\": state}\n",
                "\n",
                "    def wrap_and_get_last_index(state: list):\n",
                "        next_task = 0\n",
                "        for message in state[::-1]:\n",
                "            if isinstance(message, FunctionMessage):\n",
                "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
                "                break\n",
                "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
                "        return {\"messages\": state}\n",
                "\n",
                "    return (\n",
                "                RunnableBranch(\n",
                "                    (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
                "                    wrap_messages | planner_prompt,\n",
                "                )\n",
                "                | llm\n",
                "                | M3LXPlanParser(tools=tools)\n",
                "            )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "llm = ChatOpenAI(model=model, temperature=temperature)\n",
                "planner = create_planner(llm, tools, prompt)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# \"enumerate all detected abnormalities, given the study 57883509.\"\n",
                "example_question=\"Plot the number of paintings that depict War for each century\"\n",
                "# id= 2000\n",
                "database_schema =_get_db_schema(db_path)\n",
                "inputs = {\"question\": example_question, \"database_schema\":database_schema}\n",
                "# config = {\"configurable\": {\"thread_id\": \"xmode-2000\"}}\n",
                "inputs=[HumanMessage(content=[inputs])]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for task in planner.stream(inputs):\n",
                "    print(task[\"tool\"], task[\"args\"])\n",
                "    print(\"---\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task Fetching Unit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import time\n",
                "from concurrent.futures import ThreadPoolExecutor, wait\n",
                "from typing import Any, Dict, Iterable, List, Union\n",
                "\n",
                "from langchain_core.runnables import (\n",
                "    chain as as_runnable,\n",
                ")\n",
                "from typing_extensions import TypedDict\n",
                "\n",
                "\n",
                "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
                "    # Get all previous tool responses\n",
                "    results = {}\n",
                "    for message in messages[::-1]:\n",
                "        if isinstance(message, FunctionMessage):\n",
                "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
                "    return results\n",
                "\n",
                "\n",
                "class SchedulerInput(TypedDict):\n",
                "    messages: List[BaseMessage]\n",
                "    tasks: Iterable[Task]\n",
                "\n",
                "\n",
                "def _execute_task(task, observations, config):\n",
                "    tool_to_use = task[\"tool\"]\n",
                "    if isinstance(tool_to_use, str):\n",
                "        return tool_to_use\n",
                "    args = task[\"args\"]\n",
                "    try:\n",
                "        if isinstance(args, str):\n",
                "            resolved_args = _resolve_arg(args, observations)\n",
                "        elif isinstance(args, dict):\n",
                "            resolved_args = {\n",
                "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
                "            }\n",
                "        else:\n",
                "            # This will likely fail\n",
                "            resolved_args = args\n",
                "    except Exception as e:\n",
                "        return (\n",
                "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
                "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
                "        )\n",
                "    try:\n",
                "        return tool_to_use.invoke(resolved_args, config)\n",
                "    except Exception as e:\n",
                "        return (\n",
                "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
                "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
                "        )\n",
                "\n",
                "\n",
                "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
                "    # $1 or ${1} -> 1\n",
                "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
                "\n",
                "    def replace_match(match):\n",
                "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
                "\n",
                "        # Return the match group, in this case the index, from the string. This is the index\n",
                "        # number we get back.\n",
                "        idx = int(match.group(1))\n",
                "        return str(observations.get(idx, match.group(0)))\n",
                "\n",
                "    # For dependencies on other tasks\n",
                "    if isinstance(arg, str):\n",
                "        return re.sub(ID_PATTERN, replace_match, arg)\n",
                "    elif isinstance(arg, list):\n",
                "        return [_resolve_arg(a, observations) for a in arg]\n",
                "    else:\n",
                "        return str(arg)\n",
                "\n",
                "\n",
                "@as_runnable\n",
                "def schedule_task(task_inputs, config):\n",
                "    task: Task = task_inputs[\"task\"]\n",
                "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
                "    try:\n",
                "        observation = _execute_task(task, observations, config)\n",
                "    except Exception:\n",
                "        import traceback\n",
                "\n",
                "        observation = traceback.format_exception()  # repr(e) +\n",
                "    observations[task[\"idx\"]] = observation\n",
                "\n",
                "\n",
                "def schedule_pending_task(\n",
                "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
                "):\n",
                "    while True:\n",
                "        deps = task[\"dependencies\"]\n",
                "        if deps and (any([dep not in observations for dep in deps])):\n",
                "            # Dependencies not yet satisfied\n",
                "            time.sleep(retry_after)\n",
                "            continue\n",
                "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
                "        break\n",
                "\n",
                "\n",
                "@as_runnable\n",
                "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
                "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
                "    # For streaming, we are making a few simplifying assumption:\n",
                "    # 1. The LLM does not create cyclic dependencies\n",
                "    # 2. That the LLM will not generate tasks with future deps\n",
                "    # If this ceases to be a good assumption, you can either\n",
                "    # adjust to do a proper topological sort (not-stream)\n",
                "    # or use a more complicated data structure\n",
                "    tasks = scheduler_input[\"tasks\"]\n",
                "    args_for_tasks = {}\n",
                "    messages = scheduler_input[\"messages\"]\n",
                "    # If we are re-planning, we may have calls that depend on previous\n",
                "    # plans. Start with those.\n",
                "    observations = _get_observations(messages)\n",
                "    task_names = {}\n",
                "    originals = set(observations)\n",
                "    # ^^ We assume each task inserts a different key above to\n",
                "    # avoid race conditions...\n",
                "    futures = []\n",
                "    retry_after = 0.25  # Retry every quarter second\n",
                "    with ThreadPoolExecutor() as executor:\n",
                "        for task in tasks:\n",
                "            deps = task[\"dependencies\"]\n",
                "            task_names[task[\"idx\"]] = (\n",
                "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
                "            )\n",
                "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
                "            if (\n",
                "                # Depends on other tasks\n",
                "                deps\n",
                "                and (any([dep not in observations for dep in deps]))\n",
                "            ):\n",
                "                futures.append(\n",
                "                    executor.submit(\n",
                "                        schedule_pending_task, task, observations, retry_after\n",
                "                    )\n",
                "                )\n",
                "            else:\n",
                "                # No deps or all deps satisfied\n",
                "                # can schedule now\n",
                "                schedule_task.invoke(dict(task=task, observations=observations))\n",
                "                # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n",
                "\n",
                "        # All tasks have been submitted or enqueued\n",
                "        # Wait for them to complete\n",
                "        wait(futures)\n",
                "    # Convert observations to new tool messages to add to the state\n",
                "    new_observations = {\n",
                "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
                "        for k in sorted(observations.keys() - originals)\n",
                "    }\n",
                "    tool_messages = [\n",
                "        FunctionMessage(\n",
                "            name=name, content=str(obs), additional_kwargs={\"idx\": k, \"args\": task_args}\n",
                "        )\n",
                "        for k, (name, task_args, obs) in new_observations.items()\n",
                "    ]\n",
                "    return tool_messages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import itertools\n",
                "\n",
                "\n",
                "@as_runnable\n",
                "def plan_and_schedule(messages: List[BaseMessage], config):\n",
                "    tasks = planner.stream(messages, config)\n",
                "    # Begin executing the planner immediately\n",
                "    try:\n",
                "        tasks = itertools.chain([next(tasks)], tasks)\n",
                "    except StopIteration:\n",
                "        # Handle the case where tasks is empty.\n",
                "        tasks = iter([])\n",
                "    scheduled_tasks = schedule_tasks.invoke(\n",
                "        {\n",
                "            \"messages\": messages,\n",
                "            \"tasks\": tasks,\n",
                "        },\n",
                "        config,\n",
                "    )\n",
                "    return scheduled_tasks"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example Plan"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tool_messages = plan_and_schedule.invoke(inputs)\n",
                "tool_messages"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Joiner"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chains.openai_functions import create_structured_output_runnable\n",
                "from langchain_core.messages import AIMessage\n",
                "from langchain_core.pydantic_v1 import BaseModel, Field\n",
                "from typing import Sequence\n",
                "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union, List\n",
                "\n",
                "from langchain_core.language_models import BaseChatModel\n",
                "from langchain_core.messages import (\n",
                "    BaseMessage,\n",
                "    FunctionMessage,\n",
                "    HumanMessage,\n",
                "    SystemMessage,\n",
                ")\n",
                "\n",
                "\n",
                "from concurrent.futures import ThreadPoolExecutor, wait\n",
                "from typing import Any, Dict, Iterable, List, Union\n",
                "\n",
                "from langchain_core.runnables import (\n",
                "    chain as as_runnable,\n",
                ")\n",
                "\n",
                "\n",
                "class FinalResponse(BaseModel):\n",
                "    \"\"\"The final response/answer.\"\"\"\n",
                "\n",
                "    response: Union[str,Dict]\n",
                "\n",
                "\n",
                "class Replan(BaseModel):\n",
                "    feedback: str = Field(\n",
                "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
                "    )\n",
                "\n",
                "\n",
                "class JoinOutputs(BaseModel):\n",
                "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
                "\n",
                "    thought: str = Field(\n",
                "        description=\"The chain of thought reasoning for the selected action\"\n",
                "    )\n",
                "    action: Union[FinalResponse, Replan]\n",
                "    \n",
                "joiner_prompt=ChatPromptTemplate.from_messages(\n",
                "        [(\"system\",'''Solve a question answering task. Here are some guidelines:\n",
                "    - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\n",
                "    - Thought needs to reason about the question based on the Observations in 1-2 sentences.\n",
                "    - Ignore irrelevant action results.\n",
                "    - If the required information is present, give a concise but complete and helpful answer to the user's question.\n",
                "    - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\n",
                "    Thought: <reason about the task results and whether you have sufficient information to answer the question>\n",
                "    Action: <action to take>\n",
                "    - If an error occurs during previous actions, replan and take corrective measures to obtain the required information.\n",
                "    - Ensure that you consider errors in all the previous steps, and tries to replan accordingly.\n",
                "    - Ensure the final answer is provided in a structured format as JSON as follows:\n",
                "        {{'Summary': <concise summary of the answer>,\n",
                "         'details': <detailed explanation and supporting information>,\n",
                "         'source': <source of the information or how it was obtained>,\n",
                "         'inference':<your final inference as YES, No, or list of requested information without any extra information which you can take from the `labels` as given below>,\n",
                "         'extra explanation':<put here the extra information that you dont provide in inference >,\n",
                "         }}\n",
                "         In the `inferencer` do not provide additinal explanation or description. Put them in `extra explanation`.\n",
                "\n",
                "       \n",
                "    Available actions:\n",
                "    (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\n",
                "    (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\n",
                "    ''' ),\n",
                "        (\"user\", '{messages}'),\n",
                "        (\"assistant\", '''\n",
                "        Using the above previous actions, decide whether to replan or finish. \n",
                "        If all the required information is present, you may finish. \n",
                "        If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you. \n",
                "        '''),\n",
                "        ]\n",
                "    ).partial(\n",
                "        examples=\"\"\n",
                "    )  \n",
                "runnable = create_structured_output_runnable(JoinOutputs, llm, joiner_prompt)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
                "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
                "    if isinstance(decision.action, Replan):\n",
                "        return response + [\n",
                "            SystemMessage(\n",
                "                content=f\"Context from last attempt: {decision.action.feedback}\"\n",
                "            )\n",
                "        ]\n",
                "    else:\n",
                "        return response + [AIMessage(content=str(decision.action.response))]\n",
                "\n",
                "\n",
                "def select_recent_messages(messages: list) -> dict:\n",
                "    selected = []\n",
                "    for msg in messages[::-1]:\n",
                "        selected.append(msg)\n",
                "        if isinstance(msg, HumanMessage):\n",
                "            break\n",
                "    return {\"messages\": selected[::-1]}\n",
                "\n",
                "\n",
                "joiner = select_recent_messages | runnable | _parse_joiner_output\n",
                "\n",
                "#input_messages = inputs + tool_messages\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Example Plan and schedule"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tool_messages = plan_and_schedule.invoke(inputs)\n",
                "\n",
                "input_messages = inputs + tool_messages\n",
                "\n",
                "joiner.invoke(input_messages)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compse uning LangGraph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Dict\n",
                "\n",
                "from langgraph.graph import END, MessageGraph, START\n",
                "\n",
                "graph_builder = MessageGraph()\n",
                "\n",
                "# 1.  Define vertices\n",
                "# We defined plan_and_schedule above already\n",
                "# Assign each node to a state variable to update\n",
                "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
                "graph_builder.add_node(\"join\", joiner)\n",
                "\n",
                "\n",
                "## Define edges\n",
                "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
                "\n",
                "### This condition determines looping logic\n",
                "\n",
                "\n",
                "def should_continue(state: List[BaseMessage]):\n",
                "    if isinstance(state[-1], AIMessage):\n",
                "        return END\n",
                "    return \"plan_and_schedule\"\n",
                "\n",
                "\n",
                "graph_builder.add_conditional_edges(\n",
                "        \"join\",\n",
                "        # Next, we pass in the function that will determine which node is called next.\n",
                "        should_continue,\n",
                "        #{\"plan_and_schedule\": \"plan_and_schedule\", \"__end__\": \"__end__\"},\n",
                "    )\n",
                "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
                "chain = graph_builder.compile()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for step in chain.invoke(inputs):\n",
                "    print(step)\n",
                "    print(\"---\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "for step in chain.invoke(inputs):\n",
                "    print(step)\n",
                "    print(\"---\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "xmode",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
