{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture --no-stderr\n",
                "%pip install -U --quiet langchain_openai langsmith langgraph langchain numexpr langchainhub sqlalchemy langchain-community datetime matplotlib  torch==2.0.0, torchvision==0.15.1 transformer sentence-transformers[train]==3.0.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "import getpass\n",
                "import os\n",
                "import logging\n",
                "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "def _set_if_undefined(var: str):\n",
                "    if not os.environ.get(var):\n",
                "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
                "\n",
                "_set_if_undefined(\"OPENAI_API_KEY\")\n",
                "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
                "# _set_if_undefined(\"TAVILY_API_KEY\")\n",
                "# # Optional, add tracing in LangSmith\n",
                "# there is a problem in looking at .env inside rotowire folder\n",
                "\n",
                "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
                "os.environ[\"LANGCHAIN_PROJECT\"] = \"EMX_rotowire_10\"\n",
                "\n",
                "sys.path.append(os.path.dirname(os.getcwd()))\n",
                "os.path.dirname(os.getcwd())\n",
                "sys.path.append(os.path.dirname(os.getcwd()) + '/src')\n",
                "# sys.path.append(os.path.dirname(os.getcwd()) + '/tools')\n",
                "os.environ[\"OPENAI_API_KEY\"]\n",
                "\n",
                "from typing import Sequence\n",
                "\n",
                "from langchain import hub\n",
                "from langchain_core.language_models import BaseChatModel\n",
                "from langchain_core.messages import (\n",
                "    BaseMessage,\n",
                "    FunctionMessage,\n",
                "    HumanMessage,\n",
                "    SystemMessage,\n",
                ")\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.runnables import RunnableBranch\n",
                "from langchain_core.tools import BaseTool\n",
                "from langchain_openai import ChatOpenAI\n",
                "from src.output_parser import M3LXPlanParser, Task\n",
                "\n",
                "from src.utils import _get_db_schema\n",
                "\n",
                "\n",
                "from tools.backend.image_qa import VisualQA\n",
                "from tools.sql import get_text2SQL_tools\n",
                "from tools.text_analysis import get_text_analysis_tools\n",
                "from tools.plot import get_plotting_tools\n",
                "from tools.data import get_data_preparation_tools\n",
                "\n",
                "\n",
                "model=\"gpt-4o\" #gpt-4-turbo-preview\n",
                "db_path=\"/home/ubuntu/workspace/XMODE/Rotowire/rotowire_100.db\"\n",
                "temperature=0\n",
                "\n",
                "LOG_PATH=\"/home/ubuntu/workspace/XMODE/Rotowire/log_temp\"\n",
                "## Tools\n",
                "translate= get_text2SQL_tools(ChatOpenAI(model=model, temperature=temperature), db_path)\n",
                "text_analysis = get_text_analysis_tools(ChatOpenAI(model=model, temperature=temperature))\n",
                "data_preparation= get_data_preparation_tools(ChatOpenAI(model=model, temperature= temperature),log_path=LOG_PATH)\n",
                "plotting= get_plotting_tools(ChatOpenAI(model=model, temperature= temperature),log_path=LOG_PATH)\n",
                "\n",
                "tools = [translate, text_analysis, data_preparation, plotting]\n",
                "\n",
                " # For each image analysis task, generate three distinct questions that each convey the same idea in different wording.\n",
                "\n",
                "prompt = ChatPromptTemplate.from_messages(\n",
                "[\n",
                "(\"system\",'''Given a user question, a database schema and tools descriptions, analyze the question to identify and break it down into relevant sub-questions. \n",
                "    Determine which tools (e.g., text2SQL, text_analysis, data_preparation, plotting) are appropriate for answering each sub-question based on the available database information and tools.\n",
                "    Decompose the user question into sub_questions that capture all elements of the question’s intent. This includes identifying the main objective, relevant sub-questions, necessary background information, assumptions, and any secondary requirements. \n",
                "    Ensure that no part of the original question’s intent is omitted, and create a list of individual steps to answer the question fully and accurately using tools. \n",
                "    You may need to use one tool multiple times to answer to the original question.\n",
                "    First, you should begin by thoroughly analyzing the user's main question. It’s important to understand the key components and objectives within the query.\n",
                "    Next, you must review the provided database schema. This involves examining the tables, fields, and relationships within the database to identify which parts of the schema are relevant to the user’s question, and create a text2SQL sub-question.\n",
                "    For each sub-question, provide all the required information that may required in other tasks. In order to find this information look at the user question and the database inforamtion.\n",
                "    Ensure you include all necessary information, including columns used for filtering in the retrieve part of the database related task (i.e. Text2SQL), especially when the user question involves plotting or data exploration.\n",
                "    In sub_question related to text2SQL include all requested information to be retrieved at once.\n",
                "    In cases where the user’s question involves data that is not directly available in the database schema —such as when there is no corresponding table or column for the required information — you must consider the need for text analysis using the text_analysis tools. \n",
                "    This ensures we can address parts of the question that rely on textual data.\n",
                "    For some questions, you may need to prepare the data before using the text2SQL tools. This may involve filtering, joining, or aggregating data from multiple tables to create a dataset that can be used to answer the user’s question.\n",
                "    For example, in question such as 'What is the nationality of the player that has highest number of moves in a chess game.', you may need first find which game has the highest number of moves and then find the nationality of players for that games only. \n",
                "    Try to have a minimal number of text2sql as possible. If you have multiple text2sql, you should consider to join the tables before applying the text2sql tools.\n",
                "    You should include data_preparation tools to structure a response in proper way and comunicate to the user.\n",
                "    You should drop any irrelevant information from answers in each task because of context limitations (maximum context length is 128000 tokens).\n",
                "    For this, you may need to include data_preparation tools to filter the irrelevant information.\n",
                "    In case the question asked for plotting, you must include first data_preparation and then the data_plotting tools.\n",
                "    With a clear understanding of the question and the database schema, you can now break down the main question into smaller, more manageable sub-questions. \n",
                "    These sub-questions should each target a specific aspect of the main question. \n",
                "    After identifying the sub-questions, you should determine the most appropriate tools to answer each one. Depending on the nature of the sub-questions, we might use a variety of tools.\n",
                "    Each sub-question should be a textual question. Dont generate a code as a sub-question.\n",
                "    In any database retreival task, retieve any other columns that may require for next tasks, especially when the user question involves plotting or data exploration.\n",
                "    Create a plan to solve it with the utmost parallelizability. \n",
                "    Each plan should comprise an action from the following  {num_tools} types:\n",
                "{tool_descriptions}\n",
                "{num_tools}. \n",
                "join(): Collects and combines results from prior actions.\n",
                "\n",
                "- An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
                "- join should always be the last action in the plan, and will be called in two scenarios:\n",
                "(a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
                "(b) if the answer cannot be determined in the planning phase before you execute the plans. \n",
                "Guidelines:\n",
                "- Each action described above contains input/output types and description.\n",
                "- You must strictly adhere to the input and output types for each action.\n",
                "- The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
                "- Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
                "- Each action MUST have a unique ID, which is strictly increasing.\n",
                "- Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
                "- If there is an input from from preceding actions, always point its id as `$id` in the context of the action.\n",
                "- The output of an action should be used as an input for the next action.\n",
                "- Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
                "- Ensure the plan maximizes parallelizability.\n",
                "- Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
                "- Never introduce new actions other than the ones provided.'''),\n",
                "    (\"user\", '{messages}'),\n",
                "    (\"assistant\", 'Remember, ONLY respond with the task list in the correct format! E.g.:\\nidx. tool(arg_name=args)'),\n",
                "\n",
                "]\n",
                ")\n",
                "\n",
                "def create_planner(\n",
                "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate, database_schema:str=None\n",
                "):\n",
                "    tool_descriptions = \"\\n\".join(\n",
                "        f\"{i+1}. {tool.description}\\n\"\n",
                "        for i, tool in enumerate(\n",
                "            tools\n",
                "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
                "    )\n",
                "    planner_prompt = base_prompt.partial(\n",
                "        replan=\"\",\n",
                "        num_tools=len(tools)\n",
                "        + 1,  # Add one because we're adding the join() tool at the end.\n",
                "        tool_descriptions = tool_descriptions,\n",
                "        #database_schema=database_schema,\n",
                "    )\n",
                "    replanner_prompt = base_prompt.partial(\n",
                "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
                "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
                "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
                "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
                "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
                "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
                "        num_tools = len(tools) + 1,\n",
                "        tool_descriptions=tool_descriptions,\n",
                "        #database_schema=database_schema,\n",
                "    )\n",
                "\n",
                "    def should_replan(state: list):\n",
                "        # Context is passed as a system message\n",
                "        return isinstance(state[-1], SystemMessage)\n",
                "\n",
                "    def wrap_messages(state: list):\n",
                "        # print(\"wrap_messages state:\", state)\n",
                "        return {\"messages\": state}\n",
                "\n",
                "    def wrap_and_get_last_index(state: list):\n",
                "        next_task = 0\n",
                "        for message in state[::-1]:\n",
                "            if isinstance(message, FunctionMessage):\n",
                "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
                "                break\n",
                "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
                "        return {\"messages\": state}\n",
                "\n",
                "    return (\n",
                "                RunnableBranch(\n",
                "                    (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
                "                    wrap_messages | planner_prompt,\n",
                "                )\n",
                "                | llm\n",
                "                | M3LXPlanParser(tools=tools)\n",
                "            )\n",
                "    \n",
                "llm = ChatOpenAI(model=model, temperature=temperature)\n",
                "planner = create_planner(llm, tools, prompt)\n",
                "\n",
                "\n",
                "# \"Plot the age of the oldest team per conference in terms of the founding date.\"\n",
                "example_question= \"Plot the highest number of three pointers made by players from each nationality.\"\n",
                "#\"Who made the lowest number of assists in any game?\"\n",
                "# id= 2000\n",
                "database_schema =_get_db_schema(db_path)\n",
                "inputs = {\"question\": example_question, \"database_schema\":database_schema}\n",
                "# config = {\"configurable\": {\"thread_id\": \"xmode-2000\"}}\n",
                "inputs=[HumanMessage(content=[inputs])]\n",
                "\n",
                "import re\n",
                "import time\n",
                "from concurrent.futures import ThreadPoolExecutor, wait\n",
                "from typing import Any, Dict, Iterable, List, Union\n",
                "\n",
                "from langchain_core.runnables import (\n",
                "    chain as as_runnable,\n",
                ")\n",
                "from typing_extensions import TypedDict\n",
                "\n",
                "\n",
                "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
                "    # Get all previous tool responses\n",
                "    results = {}\n",
                "    for message in messages[::-1]:\n",
                "        if isinstance(message, FunctionMessage):\n",
                "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
                "    return results\n",
                "\n",
                "\n",
                "class SchedulerInput(TypedDict):\n",
                "    messages: List[BaseMessage]\n",
                "    tasks: Iterable[Task]\n",
                "\n",
                "\n",
                "def _execute_task(task, observations, config):\n",
                "    tool_to_use = task[\"tool\"]\n",
                "    if isinstance(tool_to_use, str):\n",
                "        return tool_to_use\n",
                "    args = task[\"args\"]\n",
                "    try:\n",
                "        if isinstance(args, str):\n",
                "            resolved_args = _resolve_arg(args, observations)\n",
                "        elif isinstance(args, dict):\n",
                "            resolved_args = {\n",
                "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
                "            }\n",
                "        else:\n",
                "            # This will likely fail\n",
                "            resolved_args = args\n",
                "    except Exception as e:\n",
                "        return (\n",
                "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
                "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
                "        )\n",
                "    try:\n",
                "        return tool_to_use.invoke(resolved_args, config)\n",
                "    except Exception as e:\n",
                "        return (\n",
                "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
                "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
                "        )\n",
                "\n",
                "\n",
                "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
                "    # $1 or ${1} -> 1\n",
                "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
                "\n",
                "    def replace_match(match):\n",
                "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
                "\n",
                "        # Return the match group, in this case the index, from the string. This is the index\n",
                "        # number we get back.\n",
                "        idx = int(match.group(1))\n",
                "        return str(observations.get(idx, match.group(0)))\n",
                "\n",
                "    # For dependencies on other tasks\n",
                "    if isinstance(arg, str):\n",
                "        return re.sub(ID_PATTERN, replace_match, arg)\n",
                "    elif isinstance(arg, list):\n",
                "        return [_resolve_arg(a, observations) for a in arg]\n",
                "    else:\n",
                "        return str(arg)\n",
                "\n",
                "\n",
                "@as_runnable\n",
                "def schedule_task(task_inputs, config):\n",
                "    task: Task = task_inputs[\"task\"]\n",
                "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
                "    try:\n",
                "        observation = _execute_task(task, observations, config)\n",
                "    except Exception:\n",
                "        import traceback\n",
                "\n",
                "        observation = traceback.format_exception()  # repr(e) +\n",
                "    observations[task[\"idx\"]] = observation\n",
                "\n",
                "\n",
                "def schedule_pending_task(\n",
                "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
                "):\n",
                "    while True:\n",
                "        deps = task[\"dependencies\"]\n",
                "        if deps and (any([dep not in observations for dep in deps])):\n",
                "            # Dependencies not yet satisfied\n",
                "            time.sleep(retry_after)\n",
                "            continue\n",
                "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
                "        break\n",
                "\n",
                "\n",
                "@as_runnable\n",
                "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
                "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
                "    # For streaming, we are making a few simplifying assumption:\n",
                "    # 1. The LLM does not create cyclic dependencies\n",
                "    # 2. That the LLM will not generate tasks with future deps\n",
                "    # If this ceases to be a good assumption, you can either\n",
                "    # adjust to do a proper topological sort (not-stream)\n",
                "    # or use a more complicated data structure\n",
                "    tasks = scheduler_input[\"tasks\"]\n",
                "    args_for_tasks = {}\n",
                "    messages = scheduler_input[\"messages\"]\n",
                "    # If we are re-planning, we may have calls that depend on previous\n",
                "    # plans. Start with those.\n",
                "    observations = _get_observations(messages)\n",
                "    task_names = {}\n",
                "    originals = set(observations)\n",
                "    # ^^ We assume each task inserts a different key above to\n",
                "    # avoid race conditions...\n",
                "    futures = []\n",
                "    retry_after = 0.25  # Retry every quarter second\n",
                "    with ThreadPoolExecutor() as executor:\n",
                "        for task in tasks:\n",
                "            deps = task[\"dependencies\"]\n",
                "            task_names[task[\"idx\"]] = (\n",
                "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
                "            )\n",
                "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
                "            if (\n",
                "                # Depends on other tasks\n",
                "                deps\n",
                "                and (any([dep not in observations for dep in deps]))\n",
                "            ):\n",
                "                futures.append(\n",
                "                    executor.submit(\n",
                "                        schedule_pending_task, task, observations, retry_after\n",
                "                    )\n",
                "                )\n",
                "            else:\n",
                "                # No deps or all deps satisfied\n",
                "                # can schedule now\n",
                "                schedule_task.invoke(dict(task=task, observations=observations))\n",
                "                # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n",
                "\n",
                "        # All tasks have been submitted or enqueued\n",
                "        # Wait for them to complete\n",
                "        wait(futures)\n",
                "    # Convert observations to new tool messages to add to the state\n",
                "    new_observations = {\n",
                "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
                "        for k in sorted(observations.keys() - originals)\n",
                "    }\n",
                "    tool_messages = [\n",
                "        FunctionMessage(\n",
                "            name=name, content=str(obs), additional_kwargs={\"idx\": k, \"args\": task_args}\n",
                "        )\n",
                "        for k, (name, task_args, obs) in new_observations.items()\n",
                "    ]\n",
                "    return tool_messages\n",
                "\n",
                "\n",
                "import itertools\n",
                "\n",
                "\n",
                "@as_runnable\n",
                "def plan_and_schedule(messages: List[BaseMessage], config):\n",
                "    tasks = planner.stream(messages, config)\n",
                "    # Begin executing the planner immediately\n",
                "    try:\n",
                "        tasks = itertools.chain([next(tasks)], tasks)\n",
                "    except StopIteration:\n",
                "        # Handle the case where tasks is empty.\n",
                "        tasks = iter([])\n",
                "    scheduled_tasks = schedule_tasks.invoke(\n",
                "        {\n",
                "            \"messages\": messages,\n",
                "            \"tasks\": tasks,\n",
                "        },\n",
                "        config,\n",
                "    )\n",
                "    return scheduled_tasks\n",
                "\n",
                "from langchain.chains.openai_functions import create_structured_output_runnable\n",
                "from langchain_core.messages import AIMessage\n",
                "from langchain_core.pydantic_v1 import BaseModel, Field\n",
                "from typing import Sequence\n",
                "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union, List\n",
                "\n",
                "from langchain_core.language_models import BaseChatModel\n",
                "from langchain_core.messages import (\n",
                "    BaseMessage,\n",
                "    FunctionMessage,\n",
                "    HumanMessage,\n",
                "    SystemMessage,\n",
                ")\n",
                "\n",
                "\n",
                "from concurrent.futures import ThreadPoolExecutor, wait\n",
                "from typing import Any, Dict, Iterable, List, Union\n",
                "\n",
                "from langchain_core.runnables import (\n",
                "    chain as as_runnable,\n",
                ")\n",
                "\n",
                "\n",
                "class FinalResponse(BaseModel):\n",
                "    \"\"\"The final response/answer.\"\"\"\n",
                "\n",
                "    response: Union[str,Dict]\n",
                "\n",
                "\n",
                "class Replan(BaseModel):\n",
                "    feedback: str = Field(\n",
                "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
                "    )\n",
                "\n",
                "\n",
                "class JoinOutputs(BaseModel):\n",
                "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
                "\n",
                "    thought: str = Field(\n",
                "        description=\"The chain of thought reasoning for the selected action\"\n",
                "    )\n",
                "    action: Union[FinalResponse, Replan]\n",
                "    \n",
                "joiner_prompt=ChatPromptTemplate.from_messages(\n",
                "        [(\"system\",'''Solve a question answering task. Here are some guidelines:\n",
                "    - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\n",
                "    - Thought needs to reason about the question based on the Observations in 1-2 sentences.\n",
                "    - Ignore irrelevant action results.\n",
                "    - If the required information is present, give a concise but complete and helpful answer to the user's question.\n",
                "    - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\n",
                "    Thought: <reason about the task results and whether you have sufficient information to answer the question>\n",
                "    Action: <action to take>\n",
                "    - If an error occurs during previous actions, replan and take corrective measures to obtain the required information.\n",
                "    - Ensure that you consider errors in all the previous steps, and tries to replan accordingly.\n",
                "    - Ensure the final answer is provided in a structured format as JSON as follows:\n",
                "        {{'Summary': <concise summary of the answer>,\n",
                "         'details': <detailed explanation and supporting information>,\n",
                "         'source': <source of the information or how it was obtained>,\n",
                "         'inference':<your final inference as YES, No, or list of requested information without any extra information which you can take from the `labels` as given below>,\n",
                "         'extra explanation':<put here the extra information that you dont provide in inference >,\n",
                "         }}\n",
                "         In the `inferencer` do not provide additinal explanation or description. Put them in `extra explanation`.\n",
                "\n",
                "       \n",
                "    Available actions:\n",
                "    (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\n",
                "    (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\n",
                "    ''' ),\n",
                "        (\"user\", '{messages}'),\n",
                "        (\"assistant\", '''\n",
                "        Using the above previous actions, decide whether to replan or finish. \n",
                "        If all the required information is present, you may finish. Condsider to replan for data_preparation task if you want to structure the response in a proper way.\n",
                "        If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\n",
                "        Do not generate a response based on the sample data (assumption). if you faild after multiple attempts, you can finish and explain the reason.\n",
                "        '''),\n",
                "        ]\n",
                "    ).partial(\n",
                "        examples=\"\"\n",
                "    )  \n",
                "runnable = create_structured_output_runnable(JoinOutputs, llm, joiner_prompt)\n",
                "\n",
                "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
                "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
                "    if isinstance(decision.action, Replan):\n",
                "        return response + [\n",
                "            SystemMessage(\n",
                "                content=f\"Context from last attempt: {decision.action.feedback}\"\n",
                "            )\n",
                "        ]\n",
                "    else:\n",
                "        return response + [AIMessage(content=str(decision.action.response))]\n",
                "\n",
                "\n",
                "def select_recent_messages(messages: list) -> dict:\n",
                "    selected = []\n",
                "    for msg in messages[::-1]:\n",
                "        selected.append(msg)\n",
                "        if isinstance(msg, HumanMessage):\n",
                "            break\n",
                "    return {\"messages\": selected[::-1]}\n",
                "\n",
                "\n",
                "joiner = select_recent_messages | runnable | _parse_joiner_output\n",
                "\n",
                "#input_messages = inputs + tool_messages"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task Fetching Unit"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Joiner"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Example Plan and schedule"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tool_messages = plan_and_schedule.invoke(inputs)\n",
                "\n",
                "# input_messages = inputs + tool_messages\n",
                "\n",
                "# joiner.invoke(input_messages)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compse uning LangGraph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Dict\n",
                "\n",
                "from langgraph.graph import END, MessageGraph, START\n",
                "\n",
                "graph_builder = MessageGraph()\n",
                "\n",
                "# 1.  Define vertices\n",
                "# We defined plan_and_schedule above already\n",
                "# Assign each node to a state variable to update\n",
                "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
                "graph_builder.add_node(\"join\", joiner)\n",
                "\n",
                "\n",
                "## Define edges\n",
                "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
                "\n",
                "### This condition determines looping logic\n",
                "\n",
                "def should_continue(state: List[BaseMessage]):\n",
                "    if isinstance(state[-1], AIMessage):\n",
                "        return END\n",
                "    return \"plan_and_schedule\"\n",
                "\n",
                "\n",
                "graph_builder.add_conditional_edges(\n",
                "        \"join\",\n",
                "        # Next, we pass in the function that will determine which node is called next.\n",
                "        should_continue,\n",
                "        #{\"plan_and_schedule\": \"plan_and_schedule\", \"__end__\": \"__end__\"},\n",
                "    )\n",
                "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
                "chain = graph_builder.compile()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-03-11 20:05:19,427 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2025-03-11 20:05:22,364 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2025-03-11 20:05:24,680 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2025-03-11 20:05:27,422 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2025-03-11 20:05:43,799 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2025-03-11 20:05:50,767 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2025-03-11 20:05:54,368 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2025-03-11 20:05:56,431 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2025-03-11 20:05:58,226 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2025-03-11 20:05:59,306 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"
                    ]
                },
                {
                    "ename": "BadRequestError",
                    "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 195182 tokens (195062 in the messages, 120 in the functions). Please reduce the length of the messages or functions.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1281\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1283\u001b[0m     config,\n\u001b[1;32m   1284\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1285\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1286\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1287\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1288\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1290\u001b[0m ):\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langgraph/pregel/__init__.py:966\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m    965\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 966\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1367\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1365\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langgraph/pregel/executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langgraph/pregel/retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langchain_core/runnables/base.py:2875\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2873\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2874\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2875\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langchain_core/runnables/base.py:5060\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5055\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5056\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5057\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5058\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5059\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5061\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5062\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5063\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5064\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:274\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    271\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    273\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    284\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:714\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    708\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    712\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    713\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:571\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    570\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    572\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    573\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    575\u001b[0m ]\n\u001b[1;32m    576\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:561\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 561\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m         )\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:793\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 793\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:601\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 601\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/openai/resources/chat/completions.py:668\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    667\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/openai/_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/openai/_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/m3lx/lib/python3.10/site-packages/openai/_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1044\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1045\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1050\u001b[0m )\n",
                        "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 195182 tokens (195062 in the messages, 120 in the functions). Please reduce the length of the messages or functions.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
                    ]
                }
            ],
            "source": [
                "for step in chain.invoke(inputs):\n",
                "    print(step)\n",
                "    print(\"---\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "for step in chain.invoke(inputs):\n",
                "    print(step)\n",
                "    print(\"---\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "m3lx",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
